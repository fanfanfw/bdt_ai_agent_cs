{% extends 'base.html' %}

{% block title %}Realtime Voice Test{% endblock %}

{% block content %}
<div class="container-fluid py-4">
    <div class="row">
        <div class="col-md-10 mx-auto">
            <!-- Header Card -->
            <div class="card shadow-lg mb-4">
                <div class="card-header bg-primary text-white">
                    <div class="d-flex justify-content-between align-items-center">
                        <div>
                            <h4 class="mb-0">
                                <i class="fas fa-microphone me-2"></i>
                                Realtime Voice Assistant Test
                            </h4>
                            <small>Speech-to-Speech with OpenAI Realtime API + RAG</small>
                        </div>
                        <div class="d-flex align-items-center">
                            <!-- Language Selector -->
                            <div class="me-3">
                                <label class="form-label text-white mb-0 me-2">Language:</label>
                                <select id="language-selector" class="form-select form-select-sm" style="width: auto; display: inline-block;">
                                    <option value="en">English</option>
                                    <option value="ms">Bahasa Malaysia</option>
                                </select>
                            </div>
                            <a href="{% url 'dashboard' %}" class="btn btn-sm btn-outline-light">
                                <i class="fas fa-arrow-left me-1"></i>Back to Dashboard
                            </a>
                        </div>
                    </div>
                </div>
                <div class="card-body">
                    <!-- Assistant Info -->
                    <div class="alert alert-info">
                        <strong>Assistant:</strong> {{ assistant.business_type.name }} Customer Service<br>
                        <strong>Knowledge Base Items:</strong> {{ assistant.knowledge_base.count }}<br>
                        <strong>Q&A Items:</strong> {{ assistant.qnas.count }}<br>
                        <strong>API:</strong> OpenAI Realtime API (Server-side WebSocket)<br>
                        <strong>Current Language:</strong> <span id="current-language-display">English</span> (Voice + Transcription)
                    </div>

                    <!-- Connection Status -->
                    <div class="row mb-4">
                        <div class="col-md-4">
                            <div class="card h-100 border-secondary">
                                <div class="card-header bg-secondary text-white text-center">
                                    <h6 class="mb-0">Connection Status</h6>
                                </div>
                                <div class="card-body text-center">
                                    <div id="connection-status" class="mb-2">
                                        <i class="fas fa-circle text-muted fa-2x"></i>
                                    </div>
                                    <div id="connection-text" class="fw-bold">Initializing...</div>
                                    <small class="text-muted">Server WebSocket</small>
                                </div>
                            </div>
                        </div>
                        <div class="col-md-4">
                            <div class="card h-100 border-info">
                                <div class="card-header bg-info text-white text-center">
                                    <h6 class="mb-0">Voice Activity</h6>
                                </div>
                                <div class="card-body text-center">
                                    <div id="voice-activity" class="mb-2">
                                        <i class="fas fa-microphone-slash text-muted fa-2x"></i>
                                    </div>
                                    <div id="voice-text" class="fw-bold">Ready</div>
                                    <small class="text-muted">Speech Detection</small>
                                </div>
                            </div>
                        </div>
                        <div class="col-md-4">
                            <div class="card h-100 border-success">
                                <div class="card-header bg-success text-white text-center">
                                    <h6 class="mb-0">AI Response</h6>
                                </div>
                                <div class="card-body text-center">
                                    <div id="ai-status" class="mb-2">
                                        <i class="fas fa-robot text-muted fa-2x"></i>
                                    </div>
                                    <div id="ai-text" class="fw-bold">Standby</div>
                                    <small class="text-muted">Model Response</small>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Main Control Panel -->
            <div class="row">
                <div class="col-md-6">
                    <div class="card shadow-lg h-100">
                        <div class="card-header bg-success text-white">
                            <h5 class="mb-0">
                                <i class="fas fa-microphone me-2"></i>Voice Control
                            </h5>
                        </div>
                        <div class="card-body text-center">
                            <!-- Voice Button -->
                            <div class="my-4">
                                <button id="voice-btn" class="btn btn-success rounded-circle" 
                                        style="width: 120px; height: 120px;" 
                                        disabled>
                                    <i class="fas fa-microphone fa-3x"></i>
                                </button>
                                <div class="mt-3">
                                    <div id="voice-btn-text" class="fw-bold fs-5">Initializing...</div>
                                    <small class="text-muted">Click to start conversation</small>
                                </div>
                            </div>

                            <!-- Conversation State -->
                            <div id="conversation-state" class="alert alert-secondary">
                                <div class="d-flex align-items-center justify-content-center">
                                    <span id="state-indicator" class="fs-4 me-2">ü§ñ</span>
                                    <span id="state-text" class="fw-bold">Getting ready...</span>
                                </div>
                            </div>


                            <!-- Transcription Preview -->
                            <div id="transcription-preview" class="alert alert-info d-none">
                                <div class="d-flex align-items-center">
                                    <span id="preview-icon" class="fs-5 me-2">üë§</span>
                                    <div>
                                        <small class="text-muted d-block">Live Transcription:</small>
                                        <span id="preview-text" class="fw-bold"></span>
                                        <span id="typing-indicator" class="text-muted">‚ñã</span>
                                    </div>
                                </div>
                            </div>

                            <!-- Instructions -->
                            <div class="alert alert-light border">
                                <h6><i class="fas fa-info-circle me-2"></i>How to use:</h6>
                                <ul class="list-unstyled mb-0 small">
                                    <li>‚Ä¢ Click the GREEN microphone button to START conversation</li>
                                    <li>‚Ä¢ Speak naturally after clicking start</li>
                                    <li>‚Ä¢ Your speech is transcribed in REAL-TIME using Whisper-1</li>
                                    <li>‚Ä¢ See live transcription preview above</li>
                                    <li>‚Ä¢ Click the RED stop button to END conversation</li>
                                    <li>‚Ä¢ Uses your knowledge base for answers</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="col-md-6">
                    <div class="card shadow-lg h-100">
                        <div class="card-header bg-info text-white d-flex justify-content-between align-items-center">
                            <h5 class="mb-0">
                                <i class="fas fa-comments me-2"></i>Conversation History
                            </h5>
                            <button class="btn btn-sm btn-outline-light" onclick="clearConversation()">
                                <i class="fas fa-trash me-1"></i>Clear
                            </button>
                        </div>
                        <div class="card-body p-0">
                            <div id="conversation-messages" class="p-3" style="height: 400px; overflow-y: auto; background-color: #f8f9fa;">
                                <!-- Welcome Message -->
                                <div class="message assistant mb-3">
                                    <div class="d-flex align-items-start">
                                        <div class="avatar me-3">
                                            <i class="fas fa-robot fa-2x text-primary"></i>
                                        </div>
                                        <div class="message-content bg-white p-3 rounded shadow-sm">
                                            <strong>{{ assistant.business_type.name }} Assistant</strong><br>
                                            Hello! I'm your realtime AI assistant. Click the microphone and start speaking - I'll respond instantly using your knowledge base!
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Debug Panel -->
            <div class="row mt-4">
                <div class="col-md-6">
                    <div class="card">
                        <div class="card-header">
                            <h6 class="mb-0">
                                <i class="fas fa-cog me-2"></i>Session Information
                            </h6>
                        </div>
                        <div class="card-body">
                            <div id="session-info" class="small">
                                <div><strong>Session ID:</strong> <span id="session-id-display">Not connected</span></div>
                                <div><strong>Status:</strong> <span id="session-status-text">Initializing</span></div>
                                <div><strong>Connection:</strong> <span>Server-side WebSocket</span></div>
                                <div><strong>Model:</strong> <span>gpt-4o-realtime-preview</span></div>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="col-md-6">
                    <div class="card">
                        <div class="card-header">
                            <h6 class="mb-0">
                                <i class="fas fa-bug me-2"></i>Debug Console
                            </h6>
                        </div>
                        <div class="card-body">
                            <div id="debug-info" class="small bg-dark text-light p-3 rounded" style="height: 150px; overflow-y: auto; font-family: monospace;">
=== Realtime Voice AI Debug Console ===
üöÄ System initializing...
‚ö° Connecting to OpenAI Realtime API...
üé§ Transcription: Using Whisper-1 model for real-time speech-to-text
üìù Live transcript preview enabled
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<!-- Toast Notifications -->
<div class="toast-container position-fixed top-0 end-0 p-3">
    <div id="notification-toast" class="toast" role="alert">
        <div class="toast-header">
            <strong class="me-auto">Voice Assistant</strong>
            <button type="button" class="btn-close" data-bs-dismiss="toast"></button>
        </div>
        <div class="toast-body" id="toast-message">
            <!-- Dynamic message content -->
        </div>
    </div>
</div>

{% endblock %}

{% block extra_css %}
<style>
#transcription-preview {
    border-left: 4px solid #17a2b8;
    background: linear-gradient(135deg, #e1f5fe, #f0f9ff);
    transition: all 0.3s ease;
}

#transcription-preview.fade-in {
    animation: fadeIn 0.3s ease-in;
}

@keyframes fadeIn {
    from { opacity: 0; transform: translateY(-10px); }
    to { opacity: 1; transform: translateY(0); }
}

#typing-indicator {
    animation: blink 1s infinite;
    color: #17a2b8;
}

@keyframes blink {
    0%, 50% { opacity: 1; }
    51%, 100% { opacity: 0.3; }
}

.message .badge {
    font-size: 0.75em;
}
</style>
{% endblock %}

{% block extra_js %}
<script>
// Configuration
const REALTIME_API_BASE = '/api/realtime-test/';
const FUNCTION_CALL_API = '/api/realtime-function-call/';

// Global state
let currentSessionId = null;
let serverWebSocket = null;
let mediaRecorder = null;
let audioStream = null;
let isConnected = false;
let isListening = false;
let isInConversation = false; // New state for conversation mode
let audioEl = null;
let selectedLanguage = 'en'; // Default language

// DOM elements
const voiceBtn = document.getElementById('voice-btn');
const voiceBtnText = document.getElementById('voice-btn-text');
const conversationMessages = document.getElementById('conversation-messages');
const sessionInfo = document.getElementById('session-info');
const debugInfo = document.getElementById('debug-info');
const sessionIdDisplay = document.getElementById('session-id-display');
const sessionStatusText = document.getElementById('session-status-text');
const languageSelector = document.getElementById('language-selector');
const currentLanguageDisplay = document.getElementById('current-language-display');

// Transcription preview elements
const transcriptionPreview = document.getElementById('transcription-preview');
const previewIcon = document.getElementById('preview-icon');
const previewText = document.getElementById('preview-text');
const typingIndicator = document.getElementById('typing-indicator');

// New conversation state elements
const conversationState = document.getElementById('conversation-state');
const stateIndicator = document.getElementById('state-indicator');
const stateText = document.getElementById('state-text');

// Status indicators
const connectionStatus = document.getElementById('connection-status');
const connectionText = document.getElementById('connection-text');
const voiceActivity = document.getElementById('voice-activity');
const voiceText = document.getElementById('voice-text');
const aiStatus = document.getElementById('ai-status');
const aiText = document.getElementById('ai-text');

// Initialize the application when DOM is loaded
document.addEventListener('DOMContentLoaded', function() {
    updateDebugInfo('üé¨ Realtime Voice AI initializing...');
    updateDebugInfo('‚ö° Powered by OpenAI Realtime API');
    
    // Setup language selector
    setupLanguageSelector();
    
    // Start initialization
    setTimeout(() => {
        initializeRealtimeVoice();
    }, 1000);
});

// Setup language selector event listener
function setupLanguageSelector() {
    if (languageSelector) {
        // Set default language
        selectedLanguage = languageSelector.value;
        
        languageSelector.addEventListener('change', function() {
            selectedLanguage = this.value;
            const langName = selectedLanguage === 'en' ? 'English' : 'Bahasa Malaysia';
            updateDebugInfo(`üåê Language switched to: ${langName}`);
            
            // Update UI text based on language
            updateUILanguage(selectedLanguage);
            
            // Update current language display
            if (currentLanguageDisplay) {
                currentLanguageDisplay.textContent = langName;
            }
            
            // If already connected, restart session with new language
            if (isConnected && isInConversation) {
                updateDebugInfo('üîÑ Restarting session with new language settings...');
                endConversation();
                setTimeout(() => {
                    startConversation();
                }, 1000);
            }
        });
        
        updateDebugInfo(`üåê Language initialized: ${selectedLanguage === 'en' ? 'English' : 'Bahasa Malaysia'}`);
    }
}

// Update UI text based on selected language
function updateUILanguage(lang) {
    const texts = {
        'en': {
            listening: 'Listening...',
            speaking: 'AI Speaking...',
            ready: 'Click the button to start',
            startConversation: 'Start Conversation',
            stopConversation: 'Stop Conversation',
            connected: 'Connected - Audio INACTIVE',
            disconnected: 'Disconnected from server'
        },
        'ms': {
            listening: 'Mendengar...',
            speaking: 'AI Bercakap...',
            ready: 'Klik butang untuk mula',
            startConversation: 'Mula Perbualan',
            stopConversation: 'Henti Perbualan',
            connected: 'Disambung - Audio TIDAK AKTIF',
            disconnected: 'Terputus dari pelayan'
        }
    };
    
    // Update button text if not in conversation
    if (!isInConversation && voiceBtnText) {
        voiceBtnText.textContent = texts[lang].startConversation;
    }
}

// Initialize realtime voice connection
async function initializeRealtimeVoice() {
    updateDebugInfo('üöÄ Starting realtime voice initialization...');
    updateConnectionStatus('connecting', 'Initializing...');
    updateStatus('session', 'processing');
    
    try {
        // Step 1: Connect to Django WebSocket server
        updateDebugInfo('üîó Connecting to Django WebSocket server...');
        await setupServerSideWebSocket();
        
    } catch (error) {
        updateDebugInfo(`‚ùå Initialization failed: ${error.message}`);
        updateConnectionStatus('error', 'Connection failed');
        updateStatus('session', 'error');
        showToast('Failed to initialize voice connection: ' + error.message, 'error');
    }
}


// Setup server-side WebSocket connection  
async function setupServerSideWebSocket() {
    updateDebugInfo('üåê Setting up server-side WebSocket connection...');
    updateConnectionStatus('connecting', 'Connecting to server...');
    updateStatus('connection', 'processing');

    try {
        // Create WebSocket connection to Django server
        const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
        const wsUrl = `${protocol}//${window.location.host}/ws/voice/default/`;
        
        updateDebugInfo(`üì° Connecting to ${wsUrl}`);
        serverWebSocket = new WebSocket(wsUrl);
        
        serverWebSocket.onopen = function(event) {
            updateDebugInfo('‚úÖ Connected to Django WebSocket server');
            isConnected = true;
            enableVoiceButton();
            updateConnectionStatus('connected', 'Connected to server - Voice INACTIVE');
            updateStatus('session', 'connected');
        };
        
        serverWebSocket.onmessage = function(event) {
            const data = JSON.parse(event.data);
            handleServerMessage(data);
        };
        
        serverWebSocket.onclose = function(event) {
            updateDebugInfo('üîå WebSocket connection closed');
            isConnected = false;
            isInConversation = false;
            updateConnectionStatus('error', 'Disconnected from server');
            voiceBtn.disabled = true;
            voiceBtnText.textContent = 'Disconnected';
        };
        
        serverWebSocket.onerror = function(error) {
            updateDebugInfo(`‚ùå WebSocket error: ${error}`);
            updateConnectionStatus('error', 'Connection error');
        };
        
        // Wait for connection
        return new Promise((resolve, reject) => {
            const timeout = setTimeout(() => {
                reject(new Error('WebSocket connection timeout'));
            }, 10000);
            
            serverWebSocket.onopen = function(event) {
                clearTimeout(timeout);
                updateDebugInfo('‚úÖ Connected to Django WebSocket server');
                isConnected = true;
                enableVoiceButton();
                updateConnectionStatus('connected', 'Connected to server - Voice INACTIVE');
                updateStatus('session', 'connected');
                resolve();
            };
        });
        
    } catch (error) {
        updateDebugInfo(`‚ùå Failed to setup WebSocket: ${error.message}`);
        updateConnectionStatus('error', 'Failed to connect to server');
        updateStatus('session', 'error');
        throw error;
    }
}

// Handle messages from Django WebSocket server
function handleServerMessage(data) {
    updateDebugInfo(`üì® Server message: ${data.type}`);
    
    switch (data.type) {
        case 'connection_status':
            updateDebugInfo(`üîó ${data.message}`);
            break;
            
        case 'voice_started':
            updateDebugInfo(`üé§ ${data.message}`);
            currentSessionId = data.session_id;
            sessionIdDisplay.textContent = currentSessionId;
            sessionStatusText.textContent = 'Server-side Voice Active';
            // Start audio recording
            startAudioRecording();
            break;
            
        case 'voice_stopped':
            updateDebugInfo(`üîá ${data.message}`);
            // Stop audio recording and clear audio queue
            stopAudioRecording();
            clearAudioQueue();
            break;
            
        case 'audio_received':
            updateDebugInfo(`üì° ${data.message}`);
            break;
            
        case 'error':
            updateDebugInfo(`‚ùå Server error: ${data.message}`);
            showToast(`Server Error: ${data.message}`, 'error');
            break;
            
        case 'ai_audio_delta':
            // Handle audio response from AI
            updateDebugInfo(`üó£Ô∏è AI Audio Delta received`);
            if (data.audio) {
                playAudioDelta(data.audio);
            }
            break;
            
        case 'ai_response_text':
            // Handle text response from AI
            updateDebugInfo(`üìù AI Response: ${data.text}`);
            addMessageToHistory('assistant', data.text, true); // Mark as transcription
            break;
            
        case 'openai_error':
            updateDebugInfo(`‚ùå OpenAI Error: ${data.error}`);
            showToast(`OpenAI Error: ${data.error}`, 'error');
            break;
            
        case 'audio_buffer_start':
            // Start buffering audio chunks
            startAudioBuffer();
            break;
            
        case 'audio_buffer_complete':
            // Complete buffering and play all chunks
            completeAudioBuffer();
            break;
            
        case 'user_transcript_delta':
            // Handle partial user transcription
            updateDebugInfo(`üë§ Transcribing: "${data.delta}"`);
            updateTranscriptionPreview('user', data.delta, false);
            updateStatus('ai', 'processing'); // Show processing while transcribing
            break;
            
        case 'user_transcript':
            // Handle complete user speech transcription
            updateDebugInfo(`‚úÖ Transcription complete: "${data.transcript}"`);
            if (data.transcript) {
                addMessageToHistory('user', data.transcript, true); // Mark as transcription
                clearTranscriptionPreview();
                updateStatus('ai', 'ready'); // Reset AI status after transcription
                showToast(`üé§ Speech transcribed: "${data.transcript.substring(0, 50)}${data.transcript.length > 50 ? '...' : ''}"`, 'success');
            }
            break;
            
        case 'user_transcript_error':
            // Handle transcription error
            updateDebugInfo(`‚ùå User transcription failed: ${JSON.stringify(data.error)}`);
            showToast(`Transcription Error: Unable to transcribe speech`, 'error');
            clearTranscriptionPreview();
            break;
            
        default:
            updateDebugInfo(`üìù Unknown message type: ${data.type}`);
            break;
    }
}

// Add message to conversation history
function addMessageToHistory(role, content, isTranscription = false) {
    const messageDiv = document.createElement('div');
    messageDiv.className = `message ${role} mb-3`;
    
    const avatar = role === 'user' ? 
        '<i class="fas fa-user fa-2x text-success"></i>' : 
        '<i class="fas fa-robot fa-2x text-primary"></i>';
    
    const alignment = role === 'user' ? 'justify-content-end' : 'align-items-start';
    const bgClass = role === 'user' ? 'bg-primary text-white' : 'bg-white';
    
    // Clean up content
    const cleanContent = content.replace(/^[^:]*:\s*/, '').trim() || content;
    
    // Add transcription indicator
    const transcriptionBadge = isTranscription ? 
        '<span class="badge bg-info me-2"><i class="fas fa-microphone me-1"></i>Transcribed</span>' : '';
    
    messageDiv.innerHTML = `
        <div class="d-flex ${alignment}">
            ${role === 'assistant' ? `<div class="avatar me-3">${avatar}</div>` : ''}
            <div class="message-content ${bgClass} p-3 rounded shadow-sm" style="max-width: 80%;">
                ${role === 'assistant' ? '<strong>AI Assistant</strong><br>' : ''}
                ${transcriptionBadge}
                ${cleanContent}
                <div class="small text-muted mt-2">
                    <i class="fas fa-clock me-1"></i>${new Date().toLocaleTimeString()}
                    <i class="fas fa-microphone ms-2 me-1"></i>Voice
                </div>
            </div>
            ${role === 'user' ? `<div class="avatar ms-3">${avatar}</div>` : ''}
        </div>
    `;
    
    conversationMessages.appendChild(messageDiv);
    conversationMessages.scrollTop = conversationMessages.scrollHeight;
}

// Status update functions
function updateConnectionStatus(status, text) {
    connectionText.textContent = text;
    const icon = connectionStatus.querySelector('i');
    
    icon.className = 'fas fa-circle fa-2x';
    
    switch (status) {
        case 'connected':
            icon.classList.add('text-success');
            break;
        case 'connecting':
            icon.classList.add('text-warning');
            break;
        case 'error':
            icon.classList.add('text-danger');
            break;
        default:
            icon.classList.add('text-muted');
    }
}

function updateStatus(type, status) {
    let statusEl, textEl, iconClass;
    
    switch (type) {
        case 'voice':
            statusEl = voiceActivity.querySelector('i');
            textEl = voiceText;
            break;
        case 'ai':
            statusEl = aiStatus.querySelector('i');
            textEl = aiText;
            break;
        default:
            return;
    }
    
    statusEl.className = 'fas fa-2x';
    
    switch (status) {
        case 'listening':
            statusEl.classList.add('fa-microphone', 'text-success');
            textEl.textContent = 'Listening';
            break;
        case 'processing':
            statusEl.classList.add('fa-spinner', 'fa-spin', 'text-warning');
            textEl.textContent = 'Processing';
            break;
        case 'speaking':
            statusEl.classList.add('fa-volume-up', 'text-info');
            textEl.textContent = 'Speaking';
            break;
        case 'ready':
            statusEl.classList.add('fa-check-circle', 'text-success');
            textEl.textContent = 'Ready';
            break;
        case 'error':
            statusEl.classList.add('fa-exclamation-triangle', 'text-danger');
            textEl.textContent = 'Error';
            break;
        default:
            statusEl.classList.add('fa-circle', 'text-muted');
            textEl.textContent = 'Standby';
    }
}

function updateDebugInfo(message) {
    const timestamp = new Date().toLocaleTimeString();
    debugInfo.textContent += `[${timestamp}] ${message}\n`;
    debugInfo.scrollTop = debugInfo.scrollHeight;
}

function enableVoiceButton() {
    voiceBtn.disabled = false;
    voiceBtn.onclick = toggleConversation;
    
    // Update button text based on language
    const texts = {
        'en': 'Start Conversation',
        'ms': 'Mula Perbualan'
    };
    voiceBtnText.textContent = texts[selectedLanguage] || texts['en'];
    
    updateConversationState('ready', 'ü§ñ', 'Click the button to start');
    updateStatus('session', 'ready');
    updateConnectionStatus('connected', 'Connected - Audio INACTIVE');
    
    // Display session info with language
    if (currentSessionId) {
        sessionIdDisplay.textContent = currentSessionId;
        sessionStatusText.textContent = `Connected via Server WebSocket (Language: ${selectedLanguage === 'en' ? 'English' : 'Bahasa Malaysia'})`;
    }
    
    const langName = selectedLanguage === 'en' ? 'English' : 'Bahasa Malaysia';
    showToast(`üéâ Connection established! Voice & Real-time Transcription ready (${langName}).`, 'success');
    updateDebugInfo(`‚úÖ Real-time transcription activated with Whisper-1 (${langName})`);
}

async function toggleConversation() {
    // Initialize audio context on user interaction
    if (!globalAudioContext) {
        try {
            globalAudioContext = new (window.AudioContext || window.webkitAudioContext)({
                sampleRate: 24000
            });
            
            if (globalAudioContext.state === 'suspended') {
                await globalAudioContext.resume();
                updateDebugInfo('üîä Audio context activated by user interaction');
            }
        } catch (error) {
            updateDebugInfo(`‚ùå Error initializing audio context: ${error.message}`);
        }
    }
    
    if (!isInConversation) {
        startConversation();
    } else {
        endConversation();
    }
}

function startConversation() {
    if (!isConnected || !serverWebSocket || serverWebSocket.readyState !== WebSocket.OPEN) {
        const errorText = selectedLanguage === 'ms' ? 
            'Sambungan pelayan tidak sedia. Sila tunggu proses permulaan selesai.' :
            'Server connection not ready. Please wait for initialization to complete.';
        showToast(errorText, 'error');
        return;
    }
    
    isInConversation = true;
    voiceBtn.classList.remove('btn-success');
    voiceBtn.classList.add('btn-danger');
    voiceBtn.innerHTML = '<i class="fas fa-stop fa-3x"></i>';
    
    const texts = {
        'en': {
            stop: 'Stop Conversation',
            starting: 'Starting voice on server...',
            requesting: 'Requesting server to start voice session...',
            toast: 'üé§ Requesting server to activate voice...'
        },
        'ms': {
            stop: 'Henti Perbualan',
            starting: 'Memulakan suara di pelayan...',
            requesting: 'Meminta pelayan untuk memulakan sesi suara...',
            toast: 'üé§ Meminta pelayan untuk mengaktifkan suara...'
        }
    };
    
    const text = texts[selectedLanguage] || texts['en'];
    voiceBtnText.textContent = text.stop;
    updateConversationState('listening', 'üëÇ', text.starting);
    updateStatus('voice', 'processing');
    updateDebugInfo(text.requesting);
    
    // Send message to Django WebSocket to start voice session with language preference
    serverWebSocket.send(JSON.stringify({
        type: 'start_voice',
        language: selectedLanguage
    }));
    
    showToast(text.toast, 'info');
}

function endConversation() {
    isInConversation = false;
    voiceBtn.classList.remove('btn-danger');
    voiceBtn.classList.add('btn-success');
    voiceBtn.innerHTML = '<i class="fas fa-microphone fa-3x"></i>';
    
    const texts = {
        'en': {
            start: 'Start Conversation',
            ready: 'Click the button to start',
            requesting: '‚úã Requesting server to stop voice session...',
            requested: 'üõë Voice DEACTIVATION requested from server',
            toast: 'üîá Requesting server to deactivate voice...'
        },
        'ms': {
            start: 'Mula Perbualan',
            ready: 'Klik butang untuk mula',
            requesting: '‚úã Meminta pelayan untuk hentikan sesi suara...',
            requested: 'üõë Penonaktifan suara diminta dari pelayan',
            toast: 'üîá Meminta pelayan untuk menyahaktifkan suara...'
        }
    };
    
    const text = texts[selectedLanguage] || texts['en'];
    voiceBtnText.textContent = text.start;
    updateConversationState('ready', 'ü§ñ', text.ready);
    updateStatus('voice', 'ready');
    updateStatus('ai', 'ready');
    updateDebugInfo(text.requesting);
    
    // Send message to Django WebSocket to stop voice session
    if (serverWebSocket && serverWebSocket.readyState === WebSocket.OPEN) {
        serverWebSocket.send(JSON.stringify({
            type: 'stop_voice'
        }));
        updateDebugInfo(text.requested);
        showToast(text.toast, 'info');
    }
}

// Audio recording and playback functions
async function startAudioRecording() {
    try {
        updateDebugInfo('üé§ Starting audio recording...');
        
        // Get microphone access
        audioStream = await navigator.mediaDevices.getUserMedia({ 
            audio: {
                sampleRate: 24000,  // OpenAI prefers 24kHz
                channelCount: 1,    // Mono
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true
            }
        });
        
        // Use AudioWorklet for real-time audio processing
        const audioContext = new (window.AudioContext || window.webkitAudioContext)({
            sampleRate: 24000  // OpenAI requires 24kHz
        });
        
        // Store reference for cleanup
        window.currentAudioContext = audioContext;
        
        const source = audioContext.createMediaStreamSource(audioStream);
        
        // Use AudioWorkletProcessor if available, fallback to ScriptProcessor
        if (audioContext.audioWorklet) {
            try {
                // Create inline AudioWorklet processor
                const processorCode = `
                    class AudioProcessor extends AudioWorkletProcessor {
                        process(inputs, outputs, parameters) {
                            const input = inputs[0];
                            if (input.length > 0) {
                                const channelData = input[0];
                                
                                // Convert float32 to int16 PCM
                                const pcm16 = new Int16Array(channelData.length);
                                for (let i = 0; i < channelData.length; i++) {
                                    pcm16[i] = Math.max(-32768, Math.min(32767, channelData[i] * 32768));
                                }
                                
                                // Send PCM data to main thread
                                this.port.postMessage({
                                    type: 'audio_data',
                                    data: pcm16.buffer
                                });
                            }
                            return true;
                        }
                    }
                    registerProcessor('audio-processor', AudioProcessor);
                `;
                
                const blob = new Blob([processorCode], { type: 'application/javascript' });
                const processorUrl = URL.createObjectURL(blob);
                
                await audioContext.audioWorklet.addModule(processorUrl);
                const processorNode = new AudioWorkletNode(audioContext, 'audio-processor');
                
                processorNode.port.onmessage = (event) => {
                    if (event.data.type === 'audio_data' && serverWebSocket && serverWebSocket.readyState === WebSocket.OPEN) {
                        // Convert ArrayBuffer to base64
                        const uint8Array = new Uint8Array(event.data.data);
                        let binaryString = '';
                        for (let i = 0; i < uint8Array.byteLength; i++) {
                            binaryString += String.fromCharCode(uint8Array[i]);
                        }
                        const base64Audio = btoa(binaryString);
                        
                        serverWebSocket.send(JSON.stringify({
                            type: 'audio_data',
                            audio: base64Audio
                        }));
                    }
                };
                
                source.connect(processorNode);
                processorNode.connect(audioContext.destination);
                
                updateDebugInfo('‚úÖ Using AudioWorklet for audio processing');
                
            } catch (error) {
                updateDebugInfo(`‚ùå AudioWorklet failed: ${error.message}, falling back to ScriptProcessor`);
                useScriptProcessor();
            }
        } else {
            updateDebugInfo('‚ö†Ô∏è AudioWorklet not supported, using ScriptProcessor');
            useScriptProcessor();
        }
        
        function useScriptProcessor() {
            const processor = audioContext.createScriptProcessor(4096, 1, 1);
            
            processor.onaudioprocess = function(e) {
                if (serverWebSocket && serverWebSocket.readyState === WebSocket.OPEN) {
                    const inputData = e.inputBuffer.getChannelData(0);
                    
                    // Convert float32 to int16 PCM
                    const pcm16 = new Int16Array(inputData.length);
                    for (let i = 0; i < inputData.length; i++) {
                        pcm16[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
                    }
                    
                    // Convert to base64
                    const uint8Array = new Uint8Array(pcm16.buffer);
                    let binaryString = '';
                    for (let i = 0; i < uint8Array.byteLength; i++) {
                        binaryString += String.fromCharCode(uint8Array[i]);
                    }
                    const base64Audio = btoa(binaryString);
                    
                    serverWebSocket.send(JSON.stringify({
                        type: 'audio_data',
                        audio: base64Audio
                    }));
                }
            };
            
            source.connect(processor);
            processor.connect(audioContext.destination);
        }
        
        updateDebugInfo('‚úÖ Audio recording started');
        updateStatus('voice', 'listening');
        updateConversationState('listening', 'üëÇ', 'Listening...');
        
    } catch (error) {
        updateDebugInfo(`‚ùå Failed to start audio recording: ${error.message}`);
        showToast(`Microphone Error: ${error.message}`, 'error');
    }
}

function stopAudioRecording() {
    try {
        updateDebugInfo('üõë Stopping audio recording...');
        
        if (audioStream) {
            audioStream.getTracks().forEach(track => track.stop());
            audioStream = null;
        }
        
        // Close audio context if it exists
        if (window.currentAudioContext) {
            window.currentAudioContext.close();
            window.currentAudioContext = null;
        }
        
        updateStatus('voice', 'ready');
        updateConversationState('ready', 'ü§ñ', 'Click the button to start');
        updateDebugInfo('‚úÖ Audio recording stopped');
        
    } catch (error) {
        updateDebugInfo(`‚ùå Error stopping audio recording: ${error.message}`);
    }
}

// Set up audio element for playback
if (!audioEl) {
    audioEl = document.createElement("audio");
    audioEl.autoplay = true;
    document.body.appendChild(audioEl);
    updateDebugInfo('üîä Audio element created for playback');
}

// Proper audio buffer management
let globalAudioContext = null;
let audioChunksBuffer = [];
let isBuffering = false;
let nextPlayTime = 0;

// Buffer audio chunks and play sequentially
async function playAudioDelta(audioData) {
    try {
        if (!audioData) return;
        
        if (isBuffering) {
            // Add to buffer during streaming
            audioChunksBuffer.push(audioData);
            updateDebugInfo(`üì¶ Buffering audio chunk (${audioData.length} chars) - Buffer size: ${audioChunksBuffer.length}`);
        } else {
            // Play immediately if not buffering (fallback)
            await playAudioChunk(audioData);
        }
        
    } catch (error) {
        updateDebugInfo(`‚ùå Error handling audio: ${error.message}`);
    }
}

async function playAudioChunk(audioData) {
    try {
        // Initialize audio context once
        if (!globalAudioContext) {
            globalAudioContext = new (window.AudioContext || window.webkitAudioContext)({
                sampleRate: 24000
            });
            
            if (globalAudioContext.state === 'suspended') {
                await globalAudioContext.resume();
                updateDebugInfo('üîä Audio context resumed');
            }
            
            nextPlayTime = globalAudioContext.currentTime;
        }
        
        // Decode base64 to binary
        const binaryString = atob(audioData);
        const bytes = new Uint8Array(binaryString.length);
        for (let i = 0; i < binaryString.length; i++) {
            bytes[i] = binaryString.charCodeAt(i);
        }
        
        // Convert PCM16 to AudioBuffer
        const pcm16Array = new Int16Array(bytes.buffer);
        
        if (pcm16Array.length > 0) {
            const audioBuffer = globalAudioContext.createBuffer(1, pcm16Array.length, 24000);
            const channelData = audioBuffer.getChannelData(0);
            
            // Convert int16 to float32
            for (let i = 0; i < pcm16Array.length; i++) {
                channelData[i] = pcm16Array[i] / 32768;
            }
            
            // Create and schedule audio source
            const source = globalAudioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(globalAudioContext.destination);
            
            // Schedule at next available time
            const startTime = Math.max(nextPlayTime, globalAudioContext.currentTime);
            source.start(startTime);
            
            // Update next play time
            nextPlayTime = startTime + audioBuffer.duration;
            
            updateDebugInfo(`‚ñ∂Ô∏è Audio chunk scheduled (${pcm16Array.length} samples) at ${startTime.toFixed(3)}s`);
        }
        
    } catch (error) {
        updateDebugInfo(`‚ùå Error playing audio chunk: ${error.message}`);
    }
}

async function startAudioBuffer() {
    updateDebugInfo('üé¨ Starting audio buffer collection');
    audioChunksBuffer = [];
    isBuffering = true;
    updateStatus('ai', 'speaking');
    updateConversationState('speaking', 'üó£Ô∏è', 'AI Speaking...');
}

async function completeAudioBuffer() {
    updateDebugInfo(`üéØ Completing audio buffer - Playing ${audioChunksBuffer.length} chunks sequentially`);
    isBuffering = false;
    
    // Play all buffered chunks in sequence
    for (const chunk of audioChunksBuffer) {
        await playAudioChunk(chunk);
    }
    
    // Clear buffer
    audioChunksBuffer = [];
    
    updateDebugInfo('‚úÖ Audio buffer playback completed');
    updateStatus('ai', 'ready');
    updateConversationState('listening', 'üëÇ', 'Listening...');
}

// Clear audio context when conversation stops
function clearAudioQueue() {
    audioChunksBuffer = [];
    isBuffering = false;
    nextPlayTime = 0;
    
    if (globalAudioContext) {
        try {
            globalAudioContext.close();
            globalAudioContext = null;
            updateDebugInfo('üîá Audio context closed');
        } catch (error) {
            updateDebugInfo(`‚ö†Ô∏è Error closing audio context: ${error.message}`);
        }
    }
    updateStatus('ai', 'ready');
    updateConversationState('listening', 'üëÇ', 'Listening...');
}

// OpenAI events are now handled server-side via Django WebSocket

function updateTranscriptionPreview(speaker, text, isComplete = false) {
    if (!text) return;
    
    transcriptionPreview.classList.remove('d-none');
    transcriptionPreview.classList.add('fade-in');
    previewIcon.textContent = speaker === 'user' ? 'üë§' : 'ü§ñ';
    previewText.textContent = text;
    
    if (isComplete) {
        typingIndicator.style.display = 'none';
        setTimeout(clearTranscriptionPreview, 2000); // Clear after 2 seconds
    } else {
        typingIndicator.style.display = 'inline';
        // Animate typing indicator
        animateTypingIndicator();
    }
}

function clearTranscriptionPreview() {
    transcriptionPreview.classList.add('d-none');
    transcriptionPreview.classList.remove('fade-in');
    previewText.textContent = '';
    typingIndicator.style.display = 'none';
}

function animateTypingIndicator() {
    const chars = ['‚ñã', '‚ñä', '‚ñâ', '‚ñà', '‚ñâ', '‚ñä'];
    let index = 0;
    
    const animate = () => {
        if (transcriptionPreview.classList.contains('d-none')) return;
        
        typingIndicator.textContent = chars[index];
        index = (index + 1) % chars.length;
        setTimeout(animate, 300);
    };
    
    animate();
}

function updateConversationState(state, icon, text) {
    conversationState.className = `alert alert-${getAlertClass(state)}`;
    stateIndicator.textContent = icon;
    stateText.textContent = text;
}

function getAlertClass(state) {
    switch (state) {
        case 'listening': return 'success';
        case 'processing': return 'warning';
        case 'speaking': return 'info';
        case 'error': return 'danger';
        default: return 'secondary';
    }
}

function clearConversation() {
    const welcomeMessage = conversationMessages.querySelector('.message.assistant');
    conversationMessages.innerHTML = '';
    if (welcomeMessage) {
        conversationMessages.appendChild(welcomeMessage);
    }
    updateDebugInfo('üßπ Conversation history cleared');
}

function showToast(message, type = 'info') {
    const toast = document.getElementById('notification-toast');
    const toastMessage = document.getElementById('toast-message');
    
    toastMessage.textContent = message;
    toast.className = `toast ${type === 'error' ? 'bg-danger text-white' : 'bg-success text-white'}`;
    
    const bsToast = new bootstrap.Toast(toast);
    bsToast.show();
}

// Utility function to get CSRF token

function getCookie(name) {
    let cookieValue = null;
    if (document.cookie && document.cookie !== '') {
        const cookies = document.cookie.split(';');
        for (let i = 0; i < cookies.length; i++) {
            const cookie = cookies[i].trim();
            if (cookie.substring(0, name.length + 1) === (name + '=')) {
                cookieValue = decodeURIComponent(cookie.substring(name.length + 1));
                break;
            }
        }
    }
    return cookieValue;
}
</script>
{% endblock %}
